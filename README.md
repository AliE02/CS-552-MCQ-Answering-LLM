# NaturalLanguagePioneers

In the context of our CS-552 class on `Modern Natural Language Processing`, we had to build an LLM capable of answering STEM related MCQs,
different fine-tuning strategies were implemented on the `Phi-3-mini-128k-instruct`.
We also implemented some variants of RAG in order to increase the model's performance.
You can find in this repo, a paper detailling all of our experiments and results, we wish you a pleasant read.

Credits to:
- Aymeric de Chillaz
- Leila Aissa
- Ali Essonni
